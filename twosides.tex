%!TEX root = main.tex

% It is hard to imagine many situations in which the experts would agree on a prior and a signal space \footnote{In the future for decisions that are largely based on analysis of biomedical images and lab results this might become feasable.}

%Ideally we would like the pool of avaialble experts to also be changing (through to obtain nontrivial bounds beyond the repeated setting it must be that this change is sufficiently slow).
%We follow some early work in this direction, but instead of searching for fairly generic bounds on the perforrmance of such mechanisms, we seek a practical mechanism for the particular case of advice under subject freedom.


While the study of decision markets so far has focused on a single decision, the motivating applications are suggestive of of a sequence of similar decisions faced by different agents:
Every day new patients with diseases are diagnosed, and treatments need to be chosen. 
A corporation faces new investment opportunities with regularity.
Scenarios such as these that motivate optimal decision elicitation are more naturally cast not as one-shot interactions, but as repeated games with many experts and a sequence of subjects who seek advice before making a decision which only affects them.
This combines the central aspects of bandits with compliance awareness (a sequence of choices and learning from past experience, where the actions of subjects are not bound to follow the algorithm choice) as well as elicitation of information from experts to enable optimal decisions without the advice being binding. 
Bounded regret algorithms with compliance awareness can be seen as addressing the special case where the experts' signals are known a priori to be uninformative, and thus only the experience can be learned from.

Our one-subject mechanism is the special case for $T={1}$, thus there is no role for exploration or learning from experience, since there are no future decisions to help inform.
A situation where experts always report their signals truthfully and have no knowledge over how to aggregate them beyond that possessed by the mechanism is equivalent to a compliance-aware contextual bandit problem. 
When contexts are constant across all time steps, the situation further reduces to a bandit problem with compliance awareness.
When the subject always follows the mechanism or $a$ cannot be observed, it reduces further  to the standard multi-armed bandit problem. 

 
%A Bayes Nash analysis, looking at the set of equilibria and comparing the worst case rewards with it to the efficient outcome, is unappealing. 
%Not only does it require imposing strong assumptions on the information structures, but also it is not clear at all that markets of this complexity will be in equilibrium.
%A compelling argument in favor of such analysis is that no-regret dynamics push to the Nash Bayes set, (cite?) 


In contrast to the previous chapter's motivation in the literature, in this chapter our focus is on constructing a practical mechanism. 
The motivation for this switch is that the setting is natural, and no mechanisms have been previously proposed.
In the two special cases of compliance aware bandits and single subject decision markets, the proposed mechanism reduces to the previously proposed mechanism.
There are, however, (infinite) other mechanisms that share this property. 

We build up to the main practical design by analysing two simplified models that illustrate the two key characteristics of our mechanism. 
First, we create the right incentives for motivating exploratory choices.
For this, the rewards from the choice of action must be linked not just to the reward during the period the action is taken, but to the full sequence of subsequent future rewards.
Second, to aggregate signals without having the mechanism or without all experts having access to a good common prior, we propose using an off-line contextual bandit algorithm to evaluate the counter-factual (marginal) value of the signals each expert provides.
We finish by presenting a mechanism that combines both ideas, and explore some of its limitations.

%(which can be taken as strong benchmark tha the bidding mechanisms can attempt to approximate).

% we also show that the the dynamic setting allows for the mechanism to do so without access to the prior. 

%and can use in fact incentivize the efficient aggregation of the signals form the expert population.  


% note if ou depedent on someone else info to show yours is useful, it si nto possible for strict dominant strategy for you (since when they report at random with the prior, you cant possibly do better than randomizing, no matter how many rounds we observe). 
% OTOH 


%Lykouris, http://arxiv.org/pdf/1505.00391.pdf
% The current best adaptive learning algorithm is a natural adaptation of the classical Hedge
% algorithm, AdaNormalHedge, due to Luo and Schapire (2015). With this framework in mind, we
% ask the following main question:
% How much rate of change can a system admit to sustain approximate efficiency, when its
% participants are adaptive learners?



\section{Model}

The game occurs over $T$ steps; at each step: 

\begin{enumerate}
\item A new subject $t$ arrives and each $i$ of $K$ experts receives a signal $s_{t,i}$ for that subject.
\item Each expert $i$ reports to the mechanism $\hat{s}_{t,i}$, and after all reports are received the mechanism selects a chosen action $c_t$
\item The subject observes $c_t$, picks an action $a_t$, and receives a reward $r_t$.
\item The mechanism provides feedback about $s_t$, $c_t$, $a_t$, and $r_t$ to experts.
\end{enumerate}
At the end of the final period the mechanism makes payments to the experts $pi_i( \cup \hat{s}_t,c_t,r_t,a_t \forall t \in T)$.


It is worth noting that having the signals represented a vector is without loss of generality.
For example, it can be a one-shot encoding of the underlying signal.
This leads to some abuse of notation, as we still denote by $s_{t,i}$ the subset of the variables encoding experts $i$'s signal in period $t$. It does, however, lead to a better mapping with the contextual bandit literature, tools from which will be crucial to our results. 

\subsection{Subjects' Beliefs and Incentives}

The previous work on incentive compatible bandits \cite{kremer2014implementing,mansour2015bayesian} has shown that there is a distribution of rewards if all agents were rational and this common knowledge, then some actions can never be explored (assuming only information revelation and no transfers can be used by the mechanism). 

Actions that a priori have lower expected rewards than all others no matter what is revealed by previous instances of other actions cannot be explored.
The logic behind this is that knowing no previous signal could persuade an agent to take the action, an agent told to take the action knows that in expectation they can do better otherwise.
That literature has largely been focused on finding information revelation strategies that are optimal, subject to the incentive constraints. 
We assume that subjects do not have direct access to the previous outcomes or recommendations, and that their beliefs over the actions of other subjects are consistent with any potential action conditional on knowledge of such actions.
That is, we do not assume common knowledge of a prior and subject rationality.
This allows us to side step the impossibility results outlined above. 

A closely related assumption (for the setting without outside experts providing advice, but with other agents also participating in a game with joint payoffs \cite{mansour2016bayesian}) are \emph{explorable actions}: the actions which some incentive-compatible policy can recommend with non-zero probability. Note that by making beliefs that there are other agents who are likely to take any actions sufficiently often enough to reveal if the rewards of that action are of the highest value, all actions can become explorable.


The motivation behind this choice is that in many settings it is natural that the experts are playing the game repeatedly and for for profit, thus rationality can be naturally achieved and sustained; this is much less likely to be the case for the subjects.
\footnote{Ample evidence from experimental economics shows that while humans in unfamiliar environments can be far from rational, experienced profesionals faced with similar real world tasks are much likelier to be rational, and this common knowledge. A vivid illustration of this is provided by experiments where chess players are faced with a centipede game.}
It is precisely because subjects are in an unfamiliar situation that they seek out the help of experts in making their decisions.
The model's main concern is thus in contrast to \cite{kremer2014implementing,mansour2015bayesian}, who take the subjects as rational and the mechanism as the social planner which learns from experience, without relying on information from rational experts.
While the assumption of rationality and common knowledge in that case enable the use of information revelation structures for incentive exploration, here we take the exploration for granted and focus on the incentives of the experts.

\section{A sequence of repeated one-shot-efficient mechanisms is inefficient}

Even with access to the prior by the mechanism (where the VCG-style mechanism provides efficiency) or in situations in which the bidding mechanism is efficient in the one-shot case, repeated use of such mechanisms fails to achieve the efficient outcome in the sequential setting.
Running the mechanisms repeatedly, once for each subject, results in choosing the arm with the maximum posterior expected reward at each step $t$ and using the payment rule:

\[
    \pi_i = \sum_1^T 
\begin{cases}
    \alpha (r - \expec[\hat{r}_{t,-i}]) ,& \text{if } \hat{c}_{t,-i} \neq c_t\\
    0,              & \text{otherwise}
\end{cases}
\]

As usual with $\alpha < 1/N$ so as to preserve rational entry by the subjects by limiting the payments.
The repeated use of single-subject-efficient mechanisms thus creates incentives for a greedy policy in the presence of multiple experts.
This is immediate from the definition of the single subject VCG-style mechanism: it selects the arm that maximizes the rewards for that period given the reports; if the reports are truthful this is the highest expected reward arm on that period.


\begin{eg}[Two Signals With Two Regimes]\label{eg:2regimes}
We consider 2 agents and 3 arms. The first arm is a safe arm with no variance and a known reward of $1/2$. The other arms have a priori a lower expected value, of $1/3$, but conditional on both agents' signals, one arm has an expected value of $2/3$ and the other of $0$. 
Each agent receives a binary signal. The optimal arm is the parity (XOR) of both signals. Let $T$ be large, such that exploration is worthwhile.
\end{eg}

In this example the greedy policy always plays the safe arm and has an expected regret of $(2/3 - 1/2)T$ relative to the optimal (fixed) contextual policy in hindsight.
Note that the optimal policy with exploration only requires 1 exploration step to identify the mapping to the best arms, thus the regret of the mechanism's BNE choice sequence relative to the optimal policy with exploration is $(2/3 - 1/2)(T-1) - (1/2-1/3) $. 

\begin{defn}[full disclosure]
  We say an optimal decision elicitation mechanism has \emph{full disclosure} if all experts receive feedback about the value of $c_t$, $a_t$, and $r_t$ in every period.
 \end{defn}


Under full disclosure and a repeated one shot VCG-like mechanism in Example~\ref{eg:2regimes}, there is a BNE of the repeated single subject VCG-style mechanism with full disclosure, which results in the greedy policy.

The same inefficiency occurs in the repeated second price auction-style mechanism.
Given that there is no winner's curse due to the signal structure, both agents bid their valuations. If the winner of the auction does not choose the safe arm, and instead explores in that period, they receive a lower payoff in expectation in that period. In future periods their bid, and by symmetry and under full disclosure the other agents' bids, are higher, since they can both now deduce the higher payoff arm; thus given the second price mechanism their payoffs are no higher in later periods. Thus exploration is not in equilibrium. 

On possible attempt to fix this would be to only reveal the outcome to the winning bidder, thus allowing them to internalize the informational advantage in future rounds payoffs, in other words by not having full disclosure.
This internalizes the benefits of explorations, yet it prevents the other experts from learning in those rounds when they do not win, severely limiting the situations in which the mechanism can be efficient.

%\NDP{Can we characterize when this inefficiency arises? it seems very general through clearly not always; basically the experts need to learn (their signals dont point apriori the optimal action but the mapping can be learnt from experience) and the signals are spread out between different experts (if there is a single one then no need to learn, but we stil getthe ineficiency from wanting to to value the singals)}

% For many problems the sequential use of mechanisms while suboptimal is not too bad in the sense that the loss of efficiency of the mechanism can be bounded relative to the optimum (see background chapter for a brief overview of such results).
% This is not the case for sequential optimal action elicitation. Repeating any efficient one shot mechanism can lead to linearly worse performance than the optimal sequential mechanism on the same problem. In Example~\ref{eg:2regimes} the regret of the  second price bidding mechanism is $0.25(T-1)$ since the first agent wins and selects the optimal arm given his information set.

% \DBA{this result (that price mechanisms can discourage exploration unless ``intellectual property'' is built in) is very nice. I imagine there's some kind of literature about this, but have no idea what formalism the IP guys would use.}
% \NDP{There is a positive externality on future time periods from exploration today, so we want to internalize those benefits for the current decision maker so as to align his incentives. It in in a line of welfare eocnomics that stretches back to 1920 formally with Pigou and the original articulation is atttributed to  Sidgwick or Marshall, but really is a bunch of 19th century economists who learned calculus more or less figured it out.

% Specifically this is a intertemporal externality which feels related to those comes form learning by doing, 
% %http://www.jstor.org/stable/2662972?seq=1#page_scan_tab_contents
% }


A different approach would be to seek a direct mechanism which internalizes exploration: a dynamic VCG-style mechanism.
However, the requirements that there be a common prior over all possible sequences of signals, actions and outcomes, and that this be known to the mechanism, make this approach completely impractical. 
From a conceptual perspective, such an approach does not shed any new light relative to what was explored in the last chapter.
Thus, we leave it unexamined, and instead focus next on a more practical mechanism.


\section{A Simple Bidding Mechanism with Exploration}

To overcome the exploration limitation of the repeated one shot mechanism, a mechanism must internalize for the decision making expert the informational benefits of exploration steps on the rewards of future periods.
This naturally motivates a mechanism that generalizes the expert bidding mechanism, by providing the expert with rewards proportional to all future periods when it wins the auction.

\begin{mech}[Bidding for Ownership of Choice (BOC) Mechanism]
An expert $i$ is the \emph{owner} at a given time period $t$ if they have won the last auction that had a winner (if no bids in a auction meet the reserve price the owner remains unchanged). 
Denote by $o_{i,t}$ an indicator variable encoding with a value of $1$ if the agent $i$ was the \emph{owner} of the choice at time $t$. 

\[
    \pi_i =  \sum_1^T
\begin{cases}
    \alpha r_t ,& \text{if } o_{i,t} = 1\\
    0,              & \text{otherwise}
\end{cases}
+
\sum_1^T
\begin{cases}
     - b_{\hat{2},t} ,& \text{if } o_{i,t} = 0 \land o_{i,t+1} = 1\\
      b_{\hat{2},t} ,& \text{if } o_{i,t}= 1 \land o_{i,t+1} = 0 \\
		0,              & \text{otherwise}
\end{cases}
\]

\end{mech}


The first part of the payments sums over the rewards for all periods during which an agent owns the rights.
The second part determines the payments when a new agent $i$ becomes the owner; they pay out the second highest bid of that period. 
When another agent takes over them as the owner, they are paid the second highest bid in that period.
Note that the reserve price can be encoded in the owner's bid in this notation, since when it wins there is no change in owner and no further payments are made. 
This linking of payments addresses the incentive problem by internalizing the positive inter-temporal information externalities created by selecting actions that have not previously been selected.


\begin{prop}
There is a BNE under which the BOC mechanism results in sublinear regret in Example~\ref{eg:2regimes}. 
\end{prop}

The optimal contextual policy with exploration has payoff of $2/3T(T-1) + 1/3$

\begin{proof}
The optimal contextual policy with exploration has payoff of $2/3T(T-1) + 1/3$. The value of the choice for an agent who controls the full sequence and observes the full set of signals is thus $\alpha (2/3T(T-1) + 1/3)$, and given the second price mechanism this can be their initial bid in a BNE. The agent explores in the first choice, and exploits in all subsequent choices. If the agent does not explore in the first choice they obtain a lower payoff. If the agent makes a lower bid they do not improve their payoff since they never win (both result in 0 payoff).
\end{proof}

%the ideal seems to be something where we just have to learn from past not aggregate. for example if all experts get the same signals and better than the subjects prior.

One problematic feature of the above is that it relies on agents' signals being truthfully reported without there being any benefit from doing so. Formally, this can be sustained in BNE because deviations do not benefit the individual making them, holding the other agent constant. Conceptually this is lacking, in that the motivation for the work is precisely to provide incentives for experts to be truthful.


%prove?
%For the special case where knowledge is  fully substituble accross at least two experts informed per actions, it incentivizes the experts to carry out the optimal amount of exploration. 

%While existing results for bayesian incentive compatible exploration and its limits still apply on the subjects part if they are all rational and this is common knowledge. For medical applications this is unlikely to be the case, with a diverse set of motivations for agents making them some far from rational, and that this will be common knowledge ammong the rational agents. Somewhat paradoxically 




% \subsection{Efficiency of the mechanism}



% Bayesian/infotheoric arguments  (?)  bound the best rate at which we could learn if we had direct accss to the experts signals since the situation then reduces those signals to context and a contextual bandit lower boundon regret (TODO ref?) applies. 

% So any notion of efficiency is relative to these lower bounds. OTOH the agents priors might be much better than the mechanism and the agentsdont need to learn the mapping. 

% (i.e. given a correct prior, how fast do we identify the optimal action? how does this vary with the concentration of the prior? , the direct mechanism is equivalent to learning it, for example under Optimally Confident UCB can we give numbers? There is the 79 bounds on the minimax rate in bnadits of gittins, does that work? note dificulty of simulations, since it involves solvng for strategies that might exploit the specific algo we propose as the initiator.



\section{Paying Only for Useful Signals Without Common Priors}

In the previous chapter we showed that in the one-shot case without a common prior it was not possible to reward only experts who provided valuable signals, since there was no way to evaluate the counterfactual reward obtained without their reports. 

Interestingly, the sequential version of the problem opens up aggregation in the repeated version of examples where doing so is not possible in the one shot-case without paying useless experts in expectation.
The reason why we need the prior in the one shot case to be able to evaluate the contribution the signal makes to picking an action with good rewards is that since we observe a single action there is no way to estimate this without using a prior. 

We now consider a setting where the experts submit their signals, however, unlike in the .
While we saw that there was a truthful equilibrium, the mechanism failed the property of paying for useless experts. We show that the repeated nature of the problem can be used to overcome payments to useless experts. 
We do this in two parts. First, reducing the problem to a contextual bandit algorithm where the context is the signals reported by all experts. Second, using the randomization used by such algorithms to construct unbiased estimators of the gain in rewards brought about by a given agent's signals, and making their reward proportional to this.
The first part allows the mechanism to learn how to use the experts' signals. The second shows how much (marginal) value they provide.

%TODO: add reference to contextual banditsin bakground chapter
%\DBA{you need to insert a mini-survey of the contextual bandit literature. As I understand it, it's only recently (Langford's last few papers) that we've gotten to the point where contextual bandits are fully usable}



Assume that the subjects arrive IID. Use an unbiased contextual bandit algorithm evaluation strategy such as \cite{li2011unbiased}  Algorithm 2 and a contextual bandit algorithm (such as \cite{syrgkanis2016efficient}) to learn to use the reported signals. %http://arxiv.org/pdf/1003.5956.pdf 
% \NDP{TODO: explicitly insert the adapted version (with probability weights in our notation)}

\begin{mech}\label{mech:bandit}[Signals without Priors]
Inputs: A contextual bandit algorithm $A$ and an unbiassed offline evaluation algorithm $E$.

%$\hat{S}_{-i} = \bigcup \hat{s}_j  \forall j \neq i \in N $ then the expected reward given the others reports is:
In each time period $t$ a new subject arrives and agents receive their signals $s_t$ and then send their reports $\hat{s_{t,i}}$. A one-shot encoding of the reports is used as context in $A$ to select an arm $c_t$, then an actual choice $a_t$ is made and reward observed $r_t$.
At the end of the last time period, for each expert $i$, estimate the loss that would be obtained by the contextual bandit algorithm without using that expert's report in its context: denote it $E(\hat{s_{-i}},A)$.
 
The payment rule is as follows:

\[
    \pi_i =  \alpha (\sum_1^T r_t -  E(\hat{s_{-i}},A))
\]

Setting $\alpha<1/N$ ex ante bounds the total payments to the experts below the benefits of having the signals.

\end{mech}


Note that when a agent $i$ doesn't change the exploration policy (in expectation), then $ \expec[r_t] -  E(\hat{s_{-i}},A)) =0  $.

While an optimal report is not guaranteed to be strictly truthful, the bound on the bias of the offline contextual estimator bounds how much a deviation from truthfulness can benefit the expert who makes it.
%TODO HOW?

%More practically, it appears very hard for a uninformed agent to obtain profits, or for a informed agent to find a strategy other than truthfulness that has higher expected reward.

%\DBA{this is very loose}.




\subsection{Limitations}


Note exploration implies learning how to interpret the signals; this, especially in the presence of multiple experts with conflicting priors, appears to be the most practical application. In other words, we are not too worried about what we lose from having to randomize the choice, since this is needed for learning. 

On the other hand, a policy that randomizes over the set of choices faces the incentive constraints on the side 

% \DBA{this is very loose. Quantify tradeoffs if possible}
On the other hand to the degree that 


This is the generalization of contextual bandits to contextual variables (signals) provided by self-interested experts who have no inherent interest in the outcome or action, but need to be incentivized to be truthful. 
To the degree that the experts know how to interpret the (full set) learning to do so is inefficient. 
%How to incorporate this appears as a fruitful avenue for future research; that is how can a prior over the joint set of signals be elicited? 




%what happened to the exploration incentives and summing over all futures? the exploraiton is taken into account by the bandit reduction, what we incentivize are reporting of signals in the form that are useful to said learner in learning the policy. 



% signals reported context to a contextual bandit.
%becuase the bandit algirthm randomizing, you have a valid instrument in that randomization, i.e. they can be used to crete an unbiased (but high variance at the edges) estimator of the rewards you would have obtained with some other infromation (i.e. the paralel bandits)


% \subsection{Incentive Compatibility for Subjects}

% One natural question given the bayesian incentive compatible bandit exploration literautre, is wether these mechanisms can work when all subjects are expected utility maximizers. If the experts bring enough information to bear, the answer is yes, and it can be so without hidding past subjects outcomes. Note however,that there are intermediate situations 

%understand the bounds in http://jmlr.csail.mit.edu/proceedings/papers/v31/agrawal13a.pdf
	


%Nasty way to solve signal manipulation for future auctins This can be side steps by dividing (endogenously) the set of experts into two, and not allowing cross bidding. Open question: is there a more elegant mechanism that does this without the separation? one naural way to do the separation is to allow the experts to see the first signal, then have them self-select into the signal or aggregation pools (they go where their return is higher, we can allow them to see where others went) .


\section{A Bid and Signal Mechanism Without Priors}



%The limitation of the one shot case, of having to pay for
The above signal-only mechanism can be potentially inefficient when there are experts who know how to map the signals to actions, and thus can help the subjects avoid some of the regret in the learning.
More broadly, experts can have additional information relative to the mechanisms that helps them aggregate the signals better but requires signals by other experts to be reported to them. 

In the previous chapter on the one-shot setting, we saw that payments for signals without a common prior that allow us to evaluate the counterfactual are problematic, in that we cannot reward useful experts more than useless ones. In the previous mechanism of this chapter, we saw that in the repeated setting this is not the case. We can use an unbiased estimator of what the rewards of a contextual bandit algorithm that didn't use a given agent's signal to fill in this counterfactual without requiring a prior.

It is worth emphasising the crucial role played in the reward function by the unbiased nature of the estimator.

This mechanism is the composition of the bid mechanism and signal mechanism.


\begin{mech}\label{mech:bidbandit}[]
Inputs: A contextual bandit algorithm $A$ and an unbiased offline evaluation algorithm $E$.

%$\hat{S}_{-i} = \bigcup \hat{s}_j  \forall j \neq i \in N $ then the expected reward given the others reports is:

The payment rule is announced.
 In each period: all agents report signals and bids to the mechanism, the mechanism displays the other experts' signals to the winner of the bidding, the winner selects the chosen action $c_t$, and this is displayed to the subject, who takes action $a_t$ and receives reward $r_t$.

At the end of the last time period, for each expert $i$, estimate the loss that would be obtained by the contextual bandit algorithm without using that expert's report in its context: denote this by $E(\hat{s_{-i}},A)$.

The payment rule is:

\[
    \pi_i = 
\alpha \sum_1^T r_t -  \expec[\sum_1^T \hat{r}_{-i,t}]
+
\sum_1^T
\begin{cases}
    \beta r_t ,& \text{if } o_{i,t} = 1\\
     0,              & \text{otherwise}
\end{cases}
+
\sum_1^T
\begin{cases}
     - b_{\hat{2},t} ,& \text{if } o_{i,t} = 0 \land o_{i,t+1} = 1\\
       b_{\hat{2},t} ,& \text{if } o_{i,t} = 1 \land o_{i,t+1} = 0 \\
	   0,              & \text{otherwise}
\end{cases}
\]

Where $\alpha$ and $\beta$ are set ex ante. 
\end{mech}


The condition that must be satisfied to make the payments from the mechanism smaller than the surplus it brings collectively to the subjects is $ \alpha + \beta < 1/2NT$.


The above algorithm is far from perfect.
The dynamic nature of the market creates a major concern that an expert would not reveal their signal truthfully and lose out on that part of the reward if they can benefit more from being the \emph{owner}.
By withholding their signal they can suppress the bids of other experts who are thus at a disadvantage; this is a particular concern since the other experts may be able to achieve higher rewards.

Consider a setting where all experts signals are symmetric and perfect complements to each other. For example, the value of the reward depends on their product.
All signals are equally valuable in the counter-factual sense used to establish rewards.
To the extent the second highest bidders value is close tot he first, there is almost no net expected value from being the owner. On the other hand, if a bidder does not report his signal truthfully, then the other bidders valuation for being the owner are 0, and the misreporting bidder can appropriate the full value of the $alpha$ part of the rewards.
Thus $\alpha$ < $\beta$ for incentive compatibility. 

Also note that the owner is restricted to using strategies with full support so that the estimator of the signal rewards can be fully evaluate. If the rewards are not IID the full support restriction must be maintained throughout all time periods. 
Thus the mechanism is inefficient in so far as the owner who knows a priori the correct policy cannot fully implemented. 

It is not clear how to prove when there is a efficient full revelation mechanism for the above mechanism, since the interaction between the owners information about how to aggregate and learn over the signals complicates the dynamic VCG styles of analysis. 


% In the one shot version of the game the BNE solution concept side steps these concerns, but they become more pressing in equilibrium. 

% Consider a situation similar to the complementary signals example, but now there are two potential ways of aggregating a period's signals into the optimal choice. %TODO add ref

% Two experts, two possible aggregation functions and two independent signals.



% A natural concern is that the experts know the estimator used by the mechanism, and can place a high bid and then make a combined set of reports and choices so the estimator's realized bias is high.
% One way to try to avoid this would be to take out the rounds where the expert $i$ won the auction when calculating the signal value of $i$; note this is biased (it could be that those situations are when the expert brings a lot of value to both the signal and the aggregation of signals, and this is indeed quite natural).







