%!TEX root = main.tex

% It is hard to imagine many situations in which the experts would agree on a prior and a signal space \footnote{In the future for decisions that are largely based on analysis of biomedical images and lab results this might become feasable.}

%Ideally we would like the pool of avaialble experts to also be changing (through to obtain nontrivial bounds beyond the repeated setting it must be that this change is sufficiently slow).
%We follow some early work in this direction, but instead of searching for fairly generic bounds on the perforrmance of such mechanisms, we seek a practical mechanism for the particular case of advice under subject freedom.


Every day there are new patients with diseases diagnosed and treatments that need to be chosen. 
The natural world scenarios that motivate the mechanism are much more naturally cast not as one shot interactions, but as repeated games with many experts and a sequence of subjects who seek advice before making a decision which only affect them.

This combines the central aspects of bandits with compliance awareness (a sequence of choices and learning from past experience, including noncompliance) and elicitation of information form experts to enable optimal decisions without the advice being bounding. 
Bounded regret algorithms with compliance awareness can be seen as addressing the special case where the experts signals are known apriori to be uninformative, and thus only the experience can be learned from. Our one-decision mechanism is the special case for $T={1}$, thus there is no role for exploration or learning from experience, since there are no future decisions  to help inform.

To create the right incentives for motivating exploratory choices, we show that the choice of action should be linked not just to the reward on the period it is taken, but to the full stream of future rewards after it. 

In contrast to the previous chapters theoretical motivation and focus on equilibria in direct mechanisms under common priors, in this chapter we focus largely on practical mechanisms, and our constructive efforts .

However to start the chapter, analizing a natural direct mechanism with common priors; which is simple the sum of payoffs and the sequence of allocations of repeating the direct one shot mechanism. The failure of this mechanism makes clear the need to link the rewards of future periods.

%(which can be taken as strong benchmark tha the bidding mechanisms can attempt to approximate).

% we also show that the the dynamic setting allows for the mechanism to do so without access to the prior. 

%and can use in fact incentivize the efficient aggregation of the signals form the expert population.  


%there seems to be a further reursion step, we are basically trying to ask the other traders how much we should pay for a given signal, but if we dnt pay then we dont get the signal, so there is some partial feedbackk? or no, it is simpler, just the full supervised setting, as long as he mechanism can commit to not using the signal for the choice but keeping it for the record?

% note if ou depedent on someone else info to show yours is useful, it si nto possible for strict dominant strategy for you (since when they report at random with the prior, you cant possibly do better than randomizing, no matter how many rounds we observe). 
% OTOH 



% The previous chapter addressed a question that emerges from the academic literature: are there efficient, budget constrained mechanisms that aggregate information to aid a decision maker? 
% It does so using tools which are relatively standard in that literature (the Bayes Nash Equilibria of a one shot game under a correct common prior), the heavy use of common priors makes them impractical.



% On a more speculative note, we present a plausibly practical mechanism for the repeated setting that is  efficient and truthfull.
%this is going to need a condition along the line of the 

%Lykouris, http://arxiv.org/pdf/1505.00391.pdf
% The current best adaptive learning algorithm is a natural adaptation of the classical Hedge
% algorithm, AdaNormalHedge, due to Luo and Schapire (2015). With this framework in mind, we
% ask the following main question:
% How much rate of change can a system admit to sustain approximate efficiency, when its
% participants are adaptive learners?



\section{Model}

The game occurs over $T$ steps, at each step: a subject $t$ arrives, $N$ experts recieve each receive a $s_{t,i}$ signal, each expert $i$ reports send a to the mechanism $\hat{s}_{t,i}$, the subject observes the action chosen by the mechanism $c_t$, picks an action $a_t$ that is actually carried out and receives a reward $r_t$.
At the end of the final period the mechanism makes payments to the experts $pi_i( \cup \hat{s}_t,c_t,r_t,a_t \forall t \in T)$.


\section{A sequence of one shot mechanisms is not efficient}

Attempting to run the single action elicitation mechanism repeatedly, that is allocating according to the max posterior reward and using the payment rule:

\[
    \pi_i = \sum_1^T \pi_{i,t} 
\]

where 
\[
    \pi_{i,t} =
\begin{cases}
    r - \expec[\hat{r}_{t,-i}] ,& \text{if } \hat{c}_{t,-i} \neq c_t\\
    0,              & \text{otherwise}
\end{cases}
\]

 at each period with payments that simply summed over all periods does not preserve efficiency.

This is because the single shot mechanism creates incentives that are equivalent to a greedy policy, it only incentivizes exploitation and not exploration: by picking at each step the optimal 
On the one hand, attempting to fix this by only revealing the outcome to the winning bidder, while internalizing the benefits of explorations, means the other experts signals are cannot be used for learning.
On the other hand, if we reveal the outcome of the round to all experts,  no agent can internalize the information revelation benefits that comes from the choice of an apriori in expectation suboptimal reward action. 

\begin{eg}[Exploration ineficiency with full revelation]
let everyone have the same prior, then upon observing the outcome, everyone has the same posterior, and thus any discovery of a higher value action will be priced in on future rounds, denying the benefits form having risked a ex-ante suboptmal action to those who take it. 
\end{eg}

\begin{eg}[Data ineficiency of selective revelation]
One agent knows which action has high reward this period, we need them to win for efficiency, the other agent has a signal which is complementary to the outocme of the first action when the ifrst action is high value, and reveals a even higher value action. We need agent 1 to win (the bidding) in the first period (and thus only they see the signal), but we need agent 2 to see the first signal to achieve efficiency in the seccond.
\end{eg}


%\section{A Direct Mechanism which internatilizes exploration}
% TODO ? you  have to take the expected value of the future rewards with the others signals but without exploration as the baseline which to reward?


\section{A Simple Bidding Mechanism which Explores}

To overcome this limitations of the exploratory policy, we attempt to construct a practical bid based mechanism that is efficient under sufficient conditions that suitably generalize those of efficient bid based mechansims in the one shot case. 

\begin{mech}

An agent $i$ is the \emph{owner} at a given time period $t$ if they have won the last auction that had a winner (if no bids in a auction meet the reserve price the owner remains unchanged). 
Denote by $o_{i,t}$ a indicator variable encoding with a value of $1$ if the agent $i$ was the \emph{owner} of the choice at time $t$. 

\[
    \pi_i =  \sum_1^T
\begin{cases}
    \zeta r_t ,& \text{if } o_{i,t} = 1\\
    \eta 0,              & \text{otherwise}
\end{cases}
+
\sum_1^T
\begin{cases}
     - b_{\hat{2},t} ,& \text{if } o_{i,t} = 0 \land o_{i,t+1} = 1\\
      b_{\hat{2},t} ,& \text{if } o_{i,t}= 1 \land o_{i,t+1} = 0 \\
		0,              & \text{otherwise}
\end{cases}
\]


The first part of payments sums over the rewards for all periods during which an agent owns the rights, the seccond part determines the payments, when a new agent $i$ becomes th owner, they pay out the seccond highest bid of that period, while when another agent takes over them as the owner they are payed the seccond highest bid in that period. Note that the reserve price can be encoded in the owners bid in this notation; since when it wins there is no change in owner no payments are made. 

\end{mech}

%prove?
%For the special case where knowledge is  fully substituble accross at least two experts informed per actions, it incentivizes the experts to carry out the optimal amount of exploration. 

%While existing results for bayesian incentive compatible exploration and its limits still apply on the subjects part if they are all rational and this is common knowledge. For medical applications this is unlikely to be the case, with a diverse set of motivations for agents making them some far from rational, and that this will be common knowledge ammong the rational agents. Somewhat paradoxically 

% Algorithmic Bayesian Persuasion http://arxiv.org/pdf/1503.05988.pdf
%  Kremer et. al. [9] introduce http://arxiv.org/pdf/1507.07191.pdf . In our main result we provide a tight bound on how many other agents can each agent observe and still have an incentive-compatible algorithm and asymptotically optimal outcome.



% \subsection{Efficiency of the mechanism}



% Bayesian/infotheoric arguments  (?)  bound the best rate at which we could learn if we had direct accss to the experts signals since the situation then reduces those signals to context and a contextual bandit lower boundon regret (TODO ref?) applies. 

% So any notion of efficiency is relative to these lower bounds. OTOH the agents priors might be much better than the mechanism and the agentsdont need to learn the mapping. 

% (i.e. given a correct prior, how fast do we identify the optimal action? how does this vary with the concentration of the prior? , the direct mechanism is equivalent to learning it, for example under Optimally Confident UCB can we give numbers? There is the 79 bounds on the minimax rate in bnadits of gittins, does that work? note dificulty of simulations, since it involves solvng for strategies that might exploit the specific algo we propose as the initiator.

 \subsection{A Sufficient Condition for Efficiency}

 \begin{lem}
 One Action per Expert is not enough for incentive compatibility
 \end{lem}

% even when each expert observes the signal that is informative of just one action, versus when the expert observes a statistic about an action (say the mean of a given arm) which can still be informative of the mean of other arms. Note in the one shot setting this doesnt hold, and ne qctin per expert is sufficient, this is becuase the seccond price auction is effective in cuting off manipulation there, but here the effect on future beliefs and thus bids in later rounds is not cut (which in one shot it is inexistent)


 \begin{lem}
 Expert signals only informative about one action at one time period (and this is common knowledge)
 \end{lem}

 Note that if a signal is informative about other acions, then it allready fails at $T=1$, as we saw in the previous chapter. The basic reason for this that there is no way for the mechanism to conbine signals of different experts beyond their projection to the price space, and cmplementary signals cannot be revealed in this manner. 

 Notice that if the signal is informative of a single action, but that information carries through to future periods, the agent if there is some probability of not winning this period, . Thus for efficiency in thi mechanism the signal has to be local to the patient.




% Note that in the one shot setting the two notions coincide

% Both the subjects rewards are idependent conditional on the treatment, and the treatments are conditionally indepedent.
% if they are not independent one can consider a situation in which if you take my bid as truthful I can gain by lying to confuse your bid in the next period, (and i buy it in the period before or during tricking you)

Informally the mechanism cannot aggregate the kinds of signals that have complemetarities (as in the one shot case with bids).


\section{A Signal Revelation Mechanism Without Common Priors}

Interestingly the sequential version of the problem opens up aggregation in the repeated version of examples where it is not possible in the one shot case without in expectation paying useless experts. 
The reason why we need the prior in the one shot case to be able to evaluate the contribution the signal makes to picking an action with good rewards; since we observe a single action there is no way to estimate this without using a prior. 

We now consider a setting where the experts submit their signals, however, unlike in the .
 While we saw that there was a revelating equilibrium, the mechanism failed the property of paying for useless experts. We show that the repeated nature of the problem can be used to overcome payments to useless experts.

We do this in two parts; by reducing the problem to a contextual bandit algorithm where the context is determined by the signals reported by all experts, and then using the randomization used by such algorithms to construct unbiased estimators of the gain in rewards brough about by a given agents signals, and making their reward proportional to this. 

Assume that the subjects arrive IID. Use an unbiased contextual bandit algorithm evlauation strategy such as \cite{li2011unbiased}  Algorithm 2. %http://arxiv.org/pdf/1003.5956.pdf 
We then evalute algorithms that do not use a given agents signal and plug those in as estimates of 


\begin{mech}\label{mech:bandit}[]


%$\hat{S}_{-i} = \bigcup \hat{s}_j  \forall j \neq i \in N $ then the expected reward given the others reports is:

After the end of the last time period, for each expert $i$, estimate the loss that would be obtained by our algorithm A without using that experts report in it's context. Since this is unbiased we can plug it in place of the seccon term $\expec[\sum_1^T \hat{r}_{-i,t}]$ in the payments.


Thus payment rule is announced as follows:

\[
    \pi_i =  \alpha \sum_1^T r_t -  \expec[\sum_1^T \hat{r}_{-i,t}]
\]

Where alpha is again set a priori. Given we are attempting not to use the prior, using some $\alpha < 1/N$ might be the best we can do.

\end{mech}


Note when a agent $i$ doesnt change the exploration policy (in expectation) then , $ \expec[r_t] - \expec[\hat{r}_{-i,t}] =0  $. 

While a truthful report is not guaranteed to be strictly truthful, the bound on the bias of the plugin payent does bound how much a deviation can benefit  and this goes to zero. More practically, it appears imposible for a uninformed agent to obtain profits, and it seems very hard for a informed agent to find a strategy other than truthfulness that has higher expected reward.


%what happened to the exploration incentives and summing over all futures? the exploraiton is taken into account by the bandit reduction, what we incentivize are reporting of signals in the form that are useful to said learner in learning the policy. 




%Note that now the winning bid agent doesnt need to have particularly good signals, but rather has to be good at aggregating the others signals; i.e. he can be the only one with access to the prior for example.
% hese two decompose nicely in the sense that the agent that is choosing, the winnr of auction, he is the one we want the logn term exploration incentives. the short term reward is internal, we want it so we know what information to stop using, but to actually make payments we can wait until the end of the mechanism.
% with "enough" time periods, we can do this

% signals reported context to a contextual bandit.
%becuase the bandit algirthm randomizing, you have a valid instrument in that randomization, i.e. they can be used to crete an unbiased (but high variance at the edges) estimator of the rewards you would have obtained with some other infromation (i.e. the paralel bandits)



\subsection{Limitations}


Note exploration implies learning how to interpret the signals, this, especially in the precence of multiple experts with conflicting priors, appears like the most practical application. I.e. not too woried about what we loose from having to randomize a little the choice, since this is needed for learing.
This is the generalization of contextual bandits to contextual variables (signals) provided by self-interested experts who have no ingerent interest in the outcome or action, but need t be incentivated to be truthful. 
To the degree the experts know how to interpret the (full set) learning to do it is ineficient. 
How to incorporate this apears as a fruitful avenue for future research; that is how can a prior over the joint set of signals be elicited? 




% \subsection{Incentive Compatibility for Subjects}

% One natural question given the bayesian incentive compatible bandit exploration literautre, is wether these mechanisms can work when all subjects are expected utility maximizers. If the experts bring enough information to bear, the answer is yes, and it can be so without hidding past subjects outcomes. Note however,that there are intermediate situations 

%understand the bounds in http://jmlr.csail.mit.edu/proceedings/papers/v31/agrawal13a.pdf
	


%Nasty way to solve signal manipulation for future auctins This can be side steps by dividing (endogenously) the set of experts into two, and not allowing cross bidding. Open question: is there a more elegant mechanism that does this without the separation? one naural way to do the separation is to allow the experts to see the first signal, then have them self-select into the signal or aggregation pools (they go where their return is higher, we can allow them to see where others went) .


\section{A bid and signal mechanism without priors}



%The limitation of the one shot case, of having to pay for
The above signal only mechanism can be potentially ineficient when there are experts who know how to map the signals to optimal mechanisms. More broadly, experts can have extra information relative to the mechanisms that helps them aggregate the signals better but requires signals by other experts to be reported to them. 

In the previous chapter on the one shot setting, we saw that payments for  signals without a common prior that allows us to evaluate the coutnerfactual are problematic, in that we cannot reward useful experts more than useless ones. In the previous mechanism of this chapter, we saw that in the repeated setting this is not the case. We can use a unbiased estimator of what the rewards of a contextual bandit algoithm that didn't use a given agents signal could be used to fill in this counterfactual without requiring a prior.

This mechanism is the composition of the bid mechanism and signal mechanism.


\begin{mech}\label{mech:bidbandit}[]


%$\hat{S}_{-i} = \bigcup \hat{s}_j  \forall j \neq i \in N $ then the expected reward given the others reports is:

The payment rule is announced. In each period: all agents report signals and bids to the mechanism, the mechanism displays the other experts signals to the winner of the bidding, the winner selects the chosen action $c_t$, this is displayed to the subject, who takes action $a_t$ and receives reward $r_t$.

After the end of the last time period, for each expert $i$, estimate the loss that would be obtained by a no regret algorithm without using that experts report in it's context, and use this as $\expec[\sum_1^T \hat{r}_{-i,t}]$ in the payments. The choice part of the payments is identical to the bidding.


Thus payment rule thus is:


\[
    \pi_i =  \sum_1^T
\begin{cases}
    \zeta r_t ,& \text{if } o_{i,t} = 1\\
     0,              & \text{otherwise}
\end{cases}
+
\sum_1^T
\begin{cases}
     - b_{\hat{2},t} ,& \text{if } o_{i,t} = 0 \land o_{i,t+1} = 1\\
       b_{\hat{2},t} ,& \text{if } o_{i,t} = 1 \land o_{i,t+1} = 0 \\
	   0,              & \text{otherwise}
\end{cases}
  +  \eta \sum_1^T r_t -  \expec[\sum_1^T \hat{r}_{-i,t}]
\]

 

Where $\zeta$ and $\eta$ are again set a priori. Given we are attempting not to use a prior distribution, using  constants such that  $\zeta + \eta < 1/2N$ .

\end{mech}


\begin{lem}[Ration Entry]

\end{lem}


 A major concern is that a expert would not reveal their signal turthfully and loose out on that part of the reward, if they can benefit more from being the \emph{owner} and by widtholding their signal they can supress bids of othe experts, this is a particular conceern since the other experts may be able to achieve higher rewards.  



 concerns;

 the experts knwo the estimator you are using, can out bid you and then make the chocies so you realized bias is high. They could 
