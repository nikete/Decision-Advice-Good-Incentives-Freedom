%!TEX root = main.tex


While the previous chapter addressed a question that emerges from the academic literature: are there efficient, budget constrained mechanisms that efficiently aggregate information to aid a decision maker, without forcing the decision makers hand to obtain good incentives for experts? It does so using tools which are relatively standard in that literature (the Bayes Nash Equilibria of a one shot game under a correct common prior), the heavy use of common priors makes them impractical. It is hard to imagine many situations in which the experts would agree on a prior and a signal space \footnote{In the future for decisions that are largely based on analysis of biomedical images and lab results this might become feasable.}

Further, the natural world scenarios that motivate the mechanism are much more naturally cast not as one shot interactions, but as repeated games with a sequence of subjects who seek advice before making some common decision. 
%Ideally we would like the pool of avaialble experts to also be changing (through to obtain nontrivial bounds beyond the repeated setting it must be that this change is sufficiently slow).
We follow some early work in this direction, but instead of searching for fairly generic bounds on the perforrmance of such mechanisms, we seek a practical mechanism for the particular case of advice under subject freedom.

We consider a sequence of subjects each eliciting advice from a fixed set of experts and choosing an action which only affect themselves directly. This combines the central aspects of bandits with compliance awareness (a sequence of choices and learning from past experience, including noncompliance) and elicitation of information form experts to enable optimal decisions without the advice being bounding. 
Bounded regret algorithms with compliance awareness can be seen as addressing the special case where the experts signals are known apriori to be uninformative, and thus only the experience can be learned from. Our one-decision mechanism is the special case for $T={1}$, thus there is no role for exploration or learning from experience, since there are no future decisions  to help inform.


To create the right incentives for motivating exploratory choices, we show that the choice of action must be linked not just to the reward on the period it is taken, but to the full stream of future rewards after it. While this is sufficient to provide a generalization under a common prior of the one shot mechanisms explored preivously, we also show that the the dynamic setting allows for the mechanism 

% note if ou depedent on someone else info to show yours is useful, it si nto possible for strict dominant strategy for you (since when they report at random with the prior, you cant possibly do better than randomizing, no matter how many rounds we observe). 
% OTOH 





On a more speculative note, we present a plausibly practical mechanism for the repeated setting that is  efficient and truthfull.
%this is going to need a condition along the line of the 

%Lykouris, http://arxiv.org/pdf/1505.00391.pdf
% The current best adaptive learning algorithm is a natural adaptation of the classical Hedge
% algorithm, AdaNormalHedge, due to Luo and Schapire (2015). With this framework in mind, we
% ask the following main question:
% How much rate of change can a system admit to sustain approximate efficiency, when its
% participants are adaptive learners?



\section{Model}

The game occurs over $T$ steps, at each step: a subject $t$ arrives, $N$ experts recieve each receive a $s_{t,i}$ signal, each expert $i$ reports send a to the mechanism $\hat{s}_{t,i}$, the subject observes the action chosen by the mechanism $c_t$, picks an action $a_t$ that is actually carried out and receives a reward $r_t$.
At the end of the final period the mechanism makes payments to the experts $pi_i( \cup \hat{s}_t,c_t,r_t,a_t \forall t \in T)$.

\section{A sequence of one shot mechanisms is not efficient}

Attempting to run the single action elicitation mechanism repeatedly, that is allocating according to the max posterior reward and using the payment rule:

\[
    \pi_i = \sum_1^T \pi_{i,t} 
\]

where 
\[
    \pi_{i,t} =
\begin{cases}
    r - E[\hat{r}_{t,-i}] ,& \text{if } \hat{c}_{t,-i} \neq c_t\\
    0,              & \text{otherwise}
\end{cases}
\]

 at each period with payments that simply summed over all periods does not preserve efficiency.

This is because the single shot mechanism creates incentives that are equivalent to a greedy policy, it only incentivizes exploitation and not exploration: by picking at each step the optimal 
On the one hand, attempting to fix this by only revealing the outcome to the winning bidder, while internalizing the benefits of explorations, means the other experts signals are cannot be used for learning.
On the other hand, if we reveal the outcome of the round to all experts,  no agent can internalize the information revelation benefits that comes from the choice of an apriori in expectation suboptimal reward action. 

\begin{eg}[Exploration ineficiency with full revelation]
let everyone have the same prior, then upon observing the outcome, everyone has the same posterior, and thus any discovery of a higher value action will be priced in on future rounds, denying the benefits form having risked a ex-ante suboptmal action to those who take it. 
\end{eg}

\begin{eg}[Data ineficiency of selective revelation]
one agent knows which action has high reward this period, we need them to win for efficiency, the other agent has a signal which is complementary to the outocme of the first action when the ifrst action is high value, and reveals a even higher value action. We need agent 1 to win (the bidding) in the first period (and thus only they see the signal), but we need agent 2 to see the first signal to achieve efficiency in the seccond.
\end{eg}


%\section{A Direct Mechanism which internatilizes exploration}
% TODO ? you  have to take the expected value of the future rewards with the others signals but without exploration as the baseline which to reward?


\section{A Practical Bid Based Mechanism which internatilizes exploration}

%\NDP{This is problematic, the BNE is not well defined }
%We first relax our notion of efficiency, as the optimal exploration policy is not in general precisely known (rather we have policies that match the best lower bounds up to TODO factors). 
%A direct efficient mechanism in the revealing BNE can be recovered, however, by a mechanism that instead of awarding based on the present reward, does it over the sequence of all future rewards. 

We use a payment rule that generalizes the seccond price bidding mechanism for the one shot case ( when $T=1$ it is equivalent) by making the payoff a share of all future periods rewards.  Unlike the repeated one shot mechanism, the mechanism auctions for the right to choose and  share the reward to \emph{all subsequent periods}, with a reserve price set by the previous winner of the auction; the seccond price (or reserve) is payed to the previous winner.  

Further, we link the different periods, by having the winner of the previous period set a reserve price for the share of the future periods at the begining of each period. We again define the final reward as a sum over per period rewards $\pi_i = \sum pi_{t,i}$. We denote by $b_{-i}$ a bid that is not by a given agent $i$. 

\[
pi_{i} =
\begin{cases}
    \alpha \sum_t^T r_t - b_{--i}, & \text{if } b_{i,t} > b_{-i,t+u} \forall -i \forall u \in \{t+1,t+2...T\}\\
    0,              & \text{otherwise}
\end{cases}
\]


Here $E[\hat{r}_{t,-i}]$ is the expected we


The compliance-aware algorithm can be used for initial rounds where a reserve price has not been met.



The mechanism consists of a seccond price auction of the right to select the sequence of chosen actions for all remaning periods, and a share of the resulting rewards.

For the special case where knowledge is  fully substituble accross at least two experts informed per actions, it incentivizes the experts to carry out the optimal amount of exploration. 

While existing results for bayesian incentive compatible exploration and its limits still apply on the subjects part if they are all rational and this is common knowledge. For medical applications this is unlikely to be the case, with a diverse set of motivations for agents making them some far from rational, and that this will be common knowledge ammong the rational agents. Somewhat paradoxically 

% Algorithmic Bayesian Persuasion http://arxiv.org/pdf/1503.05988.pdf
%  Kremer et. al. [9] introduce http://arxiv.org/pdf/1507.07191.pdf . In our main result we provide a tight bound on how many other agents can each agent observe and still have an incentive-compatible algorithm and asymptotically optimal outcome.





\section{Efficiency of the mechanism}



Bayesian/infotheoric arguments  (?)  bound the best rate at which we could learn if we had direct accss to the experts signals since the situation then reduces those signals to context and a contextual bandit lower boundon regret (TODO ref?) applies. 

So any notion of efficiency is a price of anarchy analysis relative to these lower bounds. 

(i.e. given a correct prior, how fast do we identify the optimal action? how does this vary with the concentration of the prior? , the direct mechanism is equivalent to learning it, for example under Optimally Confident UCB can we give numbers? There is the 79 bounds on the minimax rate in bnadits of gittins, does that work? note dificulty of simulations, since it involves solvng for strategies that might exploit the specific algo we propose as the initiator.

\subsection{One Action per Expert is not enough for incentive compatibility}

even when each expert observes the signal that is informative of just one action, versus when the expert observes a statistic about an action (say the mean of a given arm) which can still be informative of the mean of other arms. Note in the one shot setting this doesnt hold, and ne qctin per expert is sufficient, this is becuase the seccond price auction is effective in cuting off manipulation there, but here the effect on uture beliefs and thus bids in later rounds is not cut (while in one shot it is inexistent)

\subsection{A sufficient condition; Expert signals only informative about one action}

Both the subjects rewards are idependent conditional on the treatment, and the treatments are conditionally indepedent.

if they are not independent one can consider a situation in which if you take my bid as truthful I can gain by lying to confuse your bid in the next period, (and i buy it in the period before or during tricking you)




\section{Incentive Compatibility for Subjects}

One natural question given the bayesian incentive compatible bandit exploration literautre, is wether these mechanisms can work when all subjects are expected utility maximizers. If the experts bring enough information to bear, the answer is yes, and it can be so without hidding past subjects outcomes. Note however,that there are intermediate situations 

%understand the bounds in http://jmlr.csail.mit.edu/proceedings/papers/v31/agrawal13a.pdf


\section{A bid and signal mechanism via regret minimization}

Interestingly the sequential version of the problem opens up aggregation in the repeated version of examples where it is not possible in the one shot case. Consider a setting where the experts submit both their signals plus a bid, as in the final mechanism proposed in the previous chapter. While we saw that there was a revelating equilibrium, the mechanism failed the property of paying for useless experts. Further, there where many other equilibria where revelation does not occur. 

By learning which signals to use, we can create mechanisms that provide more intensive incentives for exerts, further we can limit the payments to useless experts, at least asymptotically. 


The basic idea is to keep track of N+1 overlapping partitions of the signal cross product space, one for each $-i$ plus the one we really use with all the signals. Pay agents a share of the difference in estimated payoff between what we did and $-i$.

Note that now the winning bid agent doesnt need to have particularly good signals, but rather has to be good at aggregating the others signals; i.e. he can be the only one with access to the prior for example.


note that the estimated share doesnt change if we replace the expectation by the realization in the all signals case (dont we we want to start also using partitions without the negative value agents? or at least alter weights in that drection?)

note that using $P$ instead of $-i$ to get us to dominant strategy fails, we still 

bidding the expected amount conditional on winning and revealing the true signal is a NE.

conjecture: regret minimization will take you there, there algorithms that if everyoen is using stil gurantee minimax and will take you there fast when all/most do use them.

Given the complexity of possible information structures,
Minimax and information aggregation, 


%can we make use of http://arxiv.org/pdf/1606.06244v2.pdf
% log barrier regularization with importance sampling that guarantees fast convergence of O(d log T/ǫ) 
% note they make no 



% Efficient Partial Monitoring with Prior Information
% Hastagiri P Vanchinathan
% Dept. of Computer Science
% ETH Zurich, Switzerland ¨
% hastagiri@inf.ethz.ch
% Gabor Bart ´ ok´
% Dept. of Computer Science
% ETH Zurich, Switzerland ¨
% bartok@inf.ethz.ch
% Andreas Krause
% Dept. of Computer Science
% ETH Zurich, Switzerland ¨
% krausea@ethz.ch
% Abstract
% Partial monitoring is a general model for online learning with limited feedback: a
% learner chooses actions in a sequential manner while an opponent chooses outcomes.
% In every round, the learner suffers some loss and receives some feedback based on
% the action and the outcome. The goal of the learner is to minimize her cumulative
% loss. Applications range from dynamic pricing to label-efficient prediction to dueling
% bandits. In this paper, we assume that we are given some prior information about the
% distribution based on which the opponent generates the outcomes. We propose BPM, a
% family of new efficient algorithms whose core is to track the outcome distribution with
% an ellipsoid centered around the estimated distribution. We show that our algorithm
% provably enjoys near-optimal regret rate for locally observable partial-monitoring
% problems against stochastic opponents. As demonstrated with experiments on synthetic
% as well as real-world data, the algorithm outperforms previous approaches, even for very
% uninformed priors, with an order of magnitude smaller regret and lower running time.



tension in the rewards; on the one hand we need a per period reward so we can run some learning algorithm and decide which information to use (and thus how valuable it is, i.e. which are the pathological reports ot get rid off), and otoh we need to incentivize exploration so we want to give the reward as a share of the full futrue stream of rewards which that exploraiton can benefit.

these two decompose nicely in the sense that the agent that is choosing, the winnr of auction, he is the one we want the logn term exploration incentives. the short term reward is internal, we want it so we know what information to stop using, but to actually make payments we can wait until the end of the mechanism.


signals reported context to a contextual bandit.

becuase the bandit algirthm randomizing, you have a valid instrument in that randomization, i.e. they can be used to crete an unbiased (but high variance at the edges) estimator of the rewards you would have obtained with some other infromation (i.e. the paralel bandits)
