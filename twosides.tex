%!TEX root = main.tex

% It is hard to imagine many situations in which the experts would agree on a prior and a signal space \footnote{In the future for decisions that are largely based on analysis of biomedical images and lab results this might become feasable.}

%Ideally we would like the pool of avaialble experts to also be changing (through to obtain nontrivial bounds beyond the repeated setting it must be that this change is sufficiently slow).
%We follow some early work in this direction, but instead of searching for fairly generic bounds on the perforrmance of such mechanisms, we seek a practical mechanism for the particular case of advice under subject freedom.


Every day there are new patients with diseases diagnosed and treatments that need to be chosen. 
The scenarios that motivate optimal decision elicitation are more naturally cast not as one shot interactions, but as repeated games with many experts and a sequence of subjects who seek advice before making a decision which only affect them.
This combines the central aspects of bandits with compliance awareness (a sequence of choices and learning from past experience, where the actions of subjects are not bound to follow the algorithm choice) and elicitation of information form experts to enable optimal decisions without the advice being bounding. 
Bounded regret algorithms with compliance awareness can be seen as addressing the special case where the experts signals are known apriori to be uninformative, and thus only the experience can be learned from.

Our one-decision mechanism is the special case for $T={1}$, thus there is no role for exploration or learning from experience, since there are no future decisions  to help inform.
When experts allways report truthfully their signals, and they have no knowledge over how to aggregate them that the subject/mechanism does have,  this is eqivalent to a compliance-aware contextual bandit problem. 
When contexts are constant accross all timesteps it further reduces to bandit problem with compliance awareness.
When subject always follows the mechanism or $a$ cannot be observed it reduces to the standard multi armed bandit problem. 

 
In contrast to the previous chapters motivation in the literature, in this chapter we switch our focus to the of constructing practical mechanisms. 
The motivation for this switch is that the setting is natural and no mechanisms exists. 

A Bayes Nash analysis, looking at the set of equilibria and comparing the worst case rewards with it to the efficient outcome, is relatively unappealing. 
Not only does it require imposing strong assumptions on the information structures, but also it is not clear at all that markets of this complexity will be in equilibrium.
One argument is that no-regret dynamics push to the Nash Bayes set, 


Theory provides a useful guide for the structure of the mechanism and it's buidling blocks.
Further, we can 
We take a compositional approach, attempting as much as possible to reuse existing algorithms and mechanisms.



We build up to the main practical design by analysing two simplified models that illustrate the two key characteristics of our mechanism. 
First , to create the right incentives for motivating exploratory choices, we show that the rewards from the choice of action should be linked not just to the reward on the period it is taken, but to the full stream of future rewards after it.
Seccond, to aggregate signals without having the mechanism or all  experts require access to a good prior, we how offline contextual bandit algorithms and their guarantees can be used to evaluate the counterfactual value of a given experts signa.
We finish by presenting a mechanism that combines both ideas, and explore some of its limitations.

%(which can be taken as strong benchmark tha the bidding mechanisms can attempt to approximate).

% we also show that the the dynamic setting allows for the mechanism to do so without access to the prior. 

%and can use in fact incentivize the efficient aggregation of the signals form the expert population.  


% note if ou depedent on someone else info to show yours is useful, it si nto possible for strict dominant strategy for you (since when they report at random with the prior, you cant possibly do better than randomizing, no matter how many rounds we observe). 
% OTOH 


%Lykouris, http://arxiv.org/pdf/1505.00391.pdf
% The current best adaptive learning algorithm is a natural adaptation of the classical Hedge
% algorithm, AdaNormalHedge, due to Luo and Schapire (2015). With this framework in mind, we
% ask the following main question:
% How much rate of change can a system admit to sustain approximate efficiency, when its
% participants are adaptive learners?



\section{Model}

The game occurs over $T$ steps, at each step : 
\begin{enumerate}
\item a new subject $t$ arrives and each $i$ of $K$ experts receives a signal $s_{t,i}$ for that subject.
\item Each expert $i$ reports to the mechanism $\hat{s}_{t,i}$, and the mechanism selects a chosen action $c_t$
\item The subject observes $c_t$, picks an action $a_t$, and receives a reward $r_t$.
\item The mechanism provides feedback about $s_t$, $c_t$, $a_t$, $r_t$ to experts.
\end{enumerate}
After the final step, the mechanism makes payments $\pi_i$ to each expert.


It is worth noting having the signals be represented a vector is without loss of generality. For example, it can be a one hot encoding of the underlying signal). It leads to some abuse of notation, as we still denote by $s_{t,i}$ the subset of the variables encoding experts $i$ signal in period $t$. It does, however, lead to a better mapping with the contextual bandit literature, tools from which will be crucial to our results. 

At the end of the final period the mechanism makes payments to the experts $pi_i( \cup \hat{s}_t,c_t,r_t,a_t \forall t \in T)$.

\section{A Sequence of repeated one-shot-efficient mechanisms is inefficient}


Even with access to the prior by the mechanism (where the VCG styl mechanisms provide efficiency) or in situations in which the bidding mechanism is efficient in the one shot case, repeated use of such mechanisms fails to achieve the efficient outcome in the sequential setting. 

Running the (single subject) efficient direct action elicitation mechanism repeatedly, results in choosing the arm with the maximum posterior expected reward at each step $t$ and using the payment rule:

\[
    \pi_i = \sum_1^T 
\begin{cases}
    \alpha (r - \expec[\hat{r}_{t,-i}]) ,& \text{if } \hat{c}_{t,-i} \neq c_t\\
    0,              & \text{otherwise}
\end{cases}
\]

As usual with $\alpha < 1/N$ so as to preserve rational entry by the subjects by limiting the payments.
The repeated use of single-subject-efficient mechanism thus is equivalent to a fully greedy policy: only exploitation and no exploration.

\begin{lem}
No exploration with truthful reports in the repeated single subject VCG style mechanism.
\end{lem}

This is inmediate from the definition of the single subject VCG style mechanism: it selects the arm that maximizes the rewards for that period given the reports, if the reports are turhtful this is the ihghest expected reward arm on that period. 

\begin{defn}[full disclosure]
We say an optimal decision elicitation mechanism has \emph{full disclosure} if all experts recieve feedback about the value of $c_t$, $a_t$ and $r_t$ in every period.
\end{defn}


\begin{eg}
3 agents, they all see signals which in expectation encode the arm with highest expectation, but they have to learn how to interpret them (which requires exploration), plus one safe arm that has higher expected reward than randomizing over the other arms (but lower than what can be obtained once you have learned to use the signals, i.e. lower than the expected rewards of the optimal policy mapping context to arms)
\end{eg}

More broadly

\NDP{There are situations (like a expert has the signal needed to unlock for each peirod the meaning of the signals) where we do get exploration, so this needs to be softened....}
\begin{lem}
No Exploration with full disclosure in the repeated one shot second price bidding mechanism.
\end{lem}



On possible attempt to fix this would be by only revealing the outcome to the winning bidder, thus allowing them to internalize the informational advantage in future rounds payoffs. 
This  internalizies the benefits of explorations, yet it prevents the other experts from learning in those rounds when they do not win, severely limiting the situations in which the mechanism can be efficient. 
More generally, partial revelation policies, where only some of the periods are revealed (in the style of  \cite{mansour2015bayesian} ) are also inefficient in general. 
 
\NDP{Can we characterize when this inneficiency arises? it seems very general through clealry not always; basically the experts need to learn (their signals dont point a priri the optimal action bt the mapping cna be learnt from experience) and the signals are spread outbetween different experts (if there is a single one)}

For many problems the sequential use of mechanisms while suboptimal is not too bad inthe sense that the loss of efficincy fo the mechanism can be bounded relative to the optimum (see background chapter for a brief overvie of such results).
This is not the case for sequential optimal action elicitaiton. Repeating any efficient one shot mechanism can lead to linearly worse performance than the optimal sequential mechanism on the same problem. In Example~\ref{eg:2regimes} the regret of the  seccond price bidding mechanism is $0.25(T-1)$ since the first agent wins and selects the optimal arm given his information set.




% \DBA{this result (that price mechanisms can discourage exploration unless ``intellectual property'' is built in) is very nice. I imagine there's some kind of literature about this, but have no idea what formalism the IP guys would use.}
% \NDP{There is a positive externality on future time periods from exploration today, so we want to internalize those benefits for the current decision maker so as to align his incentives. It in in a line of welfare eocnomics that stretches back to 1920 formally with Pigou and the original articulation is atttributed to  Sidgwick or Marshall, but really is a bunch of 19th century economists who learned calculus more or less figured it out.

% Specifically this is a intertemporal externality which feels related to those comes form learning by doing, 
% %http://www.jstor.org/stable/2662972?seq=1#page_scan_tab_contents
% }





One approach would be to seek a direct mechanism which internatilizes exploration; this is in theory relatively straightfoward, the dynamic VCG style mechansim , however, the requirements that the mechanism now have access tot he common prior over sequences makes it completly impractical.

% TODO ? you  have to take the expected value of the future rewards with the others signals but without exploration as the baseline which to reward?



\section{A Simple Bidding Mechanism with Exploration}

To overcome this limitations of the repeated mechanism, we present a bid based mechanism that is efficient under sufficient conditions that suitably generalize those of bid based mechansims in the one shot case. 

\begin{mech}[Bidding for ownership of choice Mechanism]

An expert $i$ is the \emph{owner} at a given time period $t$ if they have won the last auction that had a winner (if no bids in a auction meet the reserve price the owner remains unchanged). 
Denote by $o_{i,t}$ a indicator variable encoding with a value of $1$ if the agent $i$ was the \emph{owner} of the choice at time $t$. 

\[
    \pi_i =  \sum_1^T
\begin{cases}
    \alpha r_t ,& \text{if } o_{i,t} = 1\\
    0,              & \text{otherwise}
\end{cases}
+
\sum_1^T
\begin{cases}
     - b_{\hat{2},t} ,& \text{if } o_{i,t} = 0 \land o_{i,t+1} = 1\\
      b_{\hat{2},t} ,& \text{if } o_{i,t}= 1 \land o_{i,t+1} = 0 \\
		0,              & \text{otherwise}
\end{cases}
\]

\end{mech}


The first part of payments sums over the rewards for all periods during which an agent owns the rights, the seccond part determines the payments, when a new agent $i$ becomes th owner, they pay out the seccond highest bid of that period, while when another agent takes over them as the owner they are payed the seccond highest bid in that period. Note that the reserve price can be encoded in the owners bid in this notation; since when it wins there is no change in owner no payments are made. 

This linking of payments address the incentive problem by internalizing the positive intertemporal information externality created by selecting actions that have not previously been selected.


Returning to Example~\ref{eg:2regimes}, this mechanism in an equilibrium has bounded regret relative to the 

\begin{lem}
No Exploration (in equilibrium) with full disclosure in the repeated one shot second price bidding mechanism.
\end{lem}

\begin{proof}
Example~\ref{eg:2regimes} is a witness. If we have full disclosure then if agent 1 is the highest bidder they always plays the action with expected reward $0.75$. If one of the other agents wins and chooses to explore, they gain the same information as the non-winning other agent, and in the next period both have (in expectation) the same valuation, thus the seccond price bid is the same as the first price, and the expected payments to the agent are 0, while the payment to win over the first agent must be $\alpha 0.75$ and the expected payment $\alpha 0.5033$
\end{proof}

%the ideal seems to be something where we just have to learn from past not aggregate. for example if all experts get the same signals and better than the subjects prior.


%prove?
%For the special case where knowledge is  fully substituble accross at least two experts informed per actions, it incentivizes the experts to carry out the optimal amount of exploration. 

%While existing results for bayesian incentive compatible exploration and its limits still apply on the subjects part if they are all rational and this is common knowledge. For medical applications this is unlikely to be the case, with a diverse set of motivations for agents making them some far from rational, and that this will be common knowledge ammong the rational agents. Somewhat paradoxically 




% \subsection{Efficiency of the mechanism}



% Bayesian/infotheoric arguments  (?)  bound the best rate at which we could learn if we had direct accss to the experts signals since the situation then reduces those signals to context and a contextual bandit lower boundon regret (TODO ref?) applies. 

% So any notion of efficiency is relative to these lower bounds. OTOH the agents priors might be much better than the mechanism and the agentsdont need to learn the mapping. 

% (i.e. given a correct prior, how fast do we identify the optimal action? how does this vary with the concentration of the prior? , the direct mechanism is equivalent to learning it, for example under Optimally Confident UCB can we give numbers? There is the 79 bounds on the minimax rate in bnadits of gittins, does that work? note dificulty of simulations, since it involves solvng for strategies that might exploit the specific algo we propose as the initiator.

%  \subsection{A Sufficient Condition for Efficiency}


%  \begin{defn}
%  Expert signals only informative about one action at one time period (and this is common knowledge) \DBA{is this a lemma or an assumption?}
%  \end{defn}

%  \begin{lem}
%  One Action per Expert is not enough for incentive compatibility
%  \end{lem}

% % even when each expert observes the signal that is informative of just one action, versus when the expert observes a statistic about an action (say the mean of a given arm) which can still be informative of the mean of other arms. Note in the one shot setting this doesnt hold, and ne qctin per expert is sufficient, this is becuase the seccond price auction is effective in cuting off manipulation there, but here the effect on future beliefs and thus bids in later rounds is not cut (which in one shot it is inexistent)


%  \begin{defn}
%  Expert signals only informative about one action at one time period (and this is common knowledge) \DBA{is this a lemma or an assumption?}
%  \end{defn}


%  Note that if a signal is informative about other acions, then it allready fails at $T=1$, as we saw in the previous chapter. The basic reason for this that there is no way for the mechanism to conbine signals of different experts beyond their projection to the price space, and cmplementary signals cannot be revealed in this manner. \DBA{citation or formal explanation}

%  Notice that if the signal is informative of a single action, but that information carries through to future periods, the agent if there is some probability of not winning this period, . Thus for efficiency in thi mechanism the signal has to be local to the patient.

% Note that in the one shot setting the two notions coincide

% Both the subjects rewards are idependent conditional on the treatment, and the treatments are conditionally indepedent.
% if they are not independent one can consider a situation in which if you take my bid as truthful I can gain by lying to confuse your bid in the next period, (and i buy it in the period before or during tricking you)



\section{A Signal Revelation Mechanism Without Common Priors}

Interestingly the sequential version of the problem opens up aggregation in the repeated version of examples where it is not possible in the one shot case without in expectation paying useless experts. 
The reason why we need the prior in the one shot case to be able to evaluate the contribution the signal makes to picking an action with good rewards; since we observe a single action there is no way to estimate this without using a prior. 

We now consider a setting where the experts submit their signals, however, unlike in the .
 While we saw that there was a revelating equilibrium, the mechanism failed the property of paying for useless experts. We show that the repeated nature of the problem can be used to overcome payments to useless experts. 

We do this in two parts; by reducing the problem to a contextual bandit algorithm where the context is determined by the signals reported by all experts, and then using the randomization used by such algorithms to construct unbiased estimators of the gain in rewards brough about by a given agents signals, and making their reward proportional to this.

\DBA{you need to insert a mini-survey of the contextual bandit literature. As I understand it, it's only recently (Langford's last few papers) that we've gotten to the point where contextual bandits are fully usable}

Assume that the subjects arrive IID. Use an unbiased contextual bandit algorithm evlauation strategy such as \cite{li2011unbiased}  Algorithm 2 and a contextual bandit algorithm (such as \cite{syrgkanis2016efficient}) to learn to use the reported signals. %http://arxiv.org/pdf/1003.5956.pdf 
\NDP{TODO: explicitly insert the adapted version (with probability weights in our notation)}


\begin{mech}\label{mech:bandit}[Signals without Priors]
Inputs: A contextial bandit algorithm $A$ and an unbiassed offline evaluation algorithm $E$.

%$\hat{S}_{-i} = \bigcup \hat{s}_j  \forall j \neq i \in N $ then the expected reward given the others reports is:
In each time period $t$ a new subject arrives and agents receive their signals $s_t$ and then send their reports $\hat{s_{t,i}}$. A one-hot encoding of the reports is used as context in $A$ to selects an arm $c_t$, then an actual choice $a_t$ is made and reward observed $r_t$.
At the end of the last time period, for each expert $i$, estimate the loss that would be obtained by the contextual bandit algorithm without using that experts report in it's context, denote it $E(\hat{s_{-i}},A)$
 
The payment rule is as follows:

\[
    \pi_i =  \alpha (\sum_1^T r_t -  E(\hat{s_{-i}},A))
\]

Setting $\alpha<1/N$ ex ante bounds the total payments to the experts bellow the benefits of having the signals.

\end{mech}


Note when a agent $i$ doesnt change the exploration policy (in expectation) then , $ \expec[r_t] -  E(\hat{s_{-i}},A)) =0  $.

While a optimal report is not guaranteed to be strictly truthful, the bound on the bias of the offline contextual estimator bounds how much a deviation can benefit the 
More practically, it appears very hard for a uninformed agent to obtain profits, or for a informed agent to find a strategy other than truthfulness that has higher expected reward.

\DBA{this is very loose}.




\subsection{Limitations}


Note exploration implies learning how to interpret the signals, this, especially in the precence of multiple experts with conflicting priors, appears like the most practical application. I.e. not too woried about what we loose from having to randomize a little the choice, since this is needed for learing. 

\DBA{this is very loose. Quantify tradeoffs if possible}

This is the generalization of contextual bandits to contextual variables (signals) provided by self-interested experts who have no ingerent interest in the outcome or action, but need t be incentivated to be truthful. 
To the degree the experts know how to interpret the (full set) learning to do it is ineficient. 
How to incorporate this apears as a fruitful avenue for future research; that is how can a prior over the joint set of signals be elicited? 




%what happened to the exploration incentives and summing over all futures? the exploraiton is taken into account by the bandit reduction, what we incentivize are reporting of signals in the form that are useful to said learner in learning the policy. 



% signals reported context to a contextual bandit.
%becuase the bandit algirthm randomizing, you have a valid instrument in that randomization, i.e. they can be used to crete an unbiased (but high variance at the edges) estimator of the rewards you would have obtained with some other infromation (i.e. the paralel bandits)


% \subsection{Incentive Compatibility for Subjects}

% One natural question given the bayesian incentive compatible bandit exploration literautre, is wether these mechanisms can work when all subjects are expected utility maximizers. If the experts bring enough information to bear, the answer is yes, and it can be so without hidding past subjects outcomes. Note however,that there are intermediate situations 

%understand the bounds in http://jmlr.csail.mit.edu/proceedings/papers/v31/agrawal13a.pdf
	


%Nasty way to solve signal manipulation for future auctins This can be side steps by dividing (endogenously) the set of experts into two, and not allowing cross bidding. Open question: is there a more elegant mechanism that does this without the separation? one naural way to do the separation is to allow the experts to see the first signal, then have them self-select into the signal or aggregation pools (they go where their return is higher, we can allow them to see where others went) .


\section{A Bid and Signal Mechanism Without Priors}



%The limitation of the one shot case, of having to pay for
The above signal only mechanism can be potentially ineficient when there are experts who know how to map the signals to actions, and thus can help the subjects avoid some of the regret in the learning.
More broadly, experts can have extra information relative to the mechanisms that helps them aggregate the signals better but requires signals by other experts to be reported to them. 

In the previous chapter on the one shot setting, we saw that payments for  signals without a common prior that allows us to evaluate the coutnerfactual are problematic, in that we cannot reward useful experts more than useless ones. In the previous mechanism of this chapter, we saw that in the repeated setting this is not the case. We can use a unbiased estimator of what the rewards of a contextual bandit algoithm that didn't use a given agents signal could be used to fill in this counterfactual without requiring a prior.

This mechanism is the composition of the bid mechanism and signal mechanism.


\begin{mech}\label{mech:bidbandit}[]
Inputs: A contextial bandit algorithm $A$ and an unbiassed offline evaluation algorithm $E$.


%$\hat{S}_{-i} = \bigcup \hat{s}_j  \forall j \neq i \in N $ then the expected reward given the others reports is:

The payment rule is announced.
 In each period: all agents report signals and bids to the mechanism, the mechanism displays the other experts signals to the winner of the bidding, the winner selects the chosen action $c_t$, this is displayed to the subject, who takes action $a_t$ and receives reward $r_t$.

At the end of the last time period, for each expert $i$, estimate the loss that would be obtained by the contextual bandit algorithm without using that experts report in it's context, denote it $E(\hat{s_{-i}},A)$



The payment rule is:

\[
    \pi_i = 
\alpha \sum_1^T r_t -  \expec[\sum_1^T \hat{r}_{-i,t}]
+
\sum_1^T
\begin{cases}
    \beta r_t ,& \text{if } o_{i,t} = 1\\
     0,              & \text{otherwise}
\end{cases}
+
\sum_1^T
\begin{cases}
     - b_{\hat{2},t} ,& \text{if } o_{i,t} = 0 \land o_{i,t+1} = 1\\
       b_{\hat{2},t} ,& \text{if } o_{i,t} = 1 \land o_{i,t+1} = 0 \\
	   0,              & \text{otherwise}
\end{cases}
\]

Where $\alpha$ and $\beta$ are set ex ante. 
\end{mech}


The condition that must be satisfied to make the payments from the mechanism smaller than the surplus it brings collectively to the subjects is $ \alpha + \beta < 1/2NT$ .



 A major concern is that a expert would not reveal their signal turthfully and loose out on that part of the reward, if they can benefit more from being the \emph{owner} and by widtholding their signal they can supress bids of othe experts, this is a particular conceern since the other experts may be able to achieve higher rewards.

A natural concern is that the experts know the estimator the mechanism is using, and can place a high bid and then make a combined set of reports and chocies so the estimators realized bias is high.
One way to try to avoid this would be to take out the rounds where the expert $i$ won the auction when calculating the signal value of $i$, note this is biased (it could be that those are when he brings a lot of value to both the signal and the aggregation of signals, and this is indeed quite natural)






