%!TEX root = main.tex

% It is hard to imagine many situations in which the experts would agree on a prior and a signal space \footnote{In the future for decisions that are largely based on analysis of biomedical images and lab results this might become feasable.}

%Ideally we would like the pool of avaialble experts to also be changing (through to obtain nontrivial bounds beyond the repeated setting it must be that this change is sufficiently slow).
%We follow some early work in this direction, but instead of searching for fairly generic bounds on the perforrmance of such mechanisms, we seek a practical mechanism for the particular case of advice under subject freedom.


Every day there are new patients with diseases diagnosed and treatments that need to be chosen. 
The scenarios that motivate optimal decision elicitation are more naturally cast not as one shot interactions, but as repeated games with many experts and a sequence of subjects who seek advice before making a decision which only affect them.
This combines the central aspects of bandits with compliance awareness (a sequence of choices and learning from past experience, where the actions of subjects are not bound to follow the algorithm choice) and elicitation of information form experts to enable optimal decisions without the advice being bounding. 
Bounded regret algorithms with compliance awareness can be seen as addressing the special case where the experts signals are known apriori to be uninformative, and thus only the experience can be learned from.

Our one-decision mechanism is the special case for $T={1}$, thus there is no role for exploration or learning from experience, since there are no future decisions  to help inform.
When experts allways report truthfully their signals, and they have no knowledge over how to aggregate them that the subject/mechanism does have,  this is eqivalent to a compliance-aware contextual bandit problem. 
When contexts are constant accross all timesteps it further reduces to bandit problem with compliance awareness.
When subject always follows the mechanism or $a$ cannot be observed it reduces to the standard multi armed bandit problem. 

 
In contrast to the previous chapters motivation in the literature, in this chapter we switch our focus to the of constructing practical mechanisms. 
The motivation for this switch is that the setting is natural and no mechanisms exists. 

A Bayes Nash analysis, looking at the set of equilibria and comparing the worst case rewards with it to the efficient outcome, is relatively unappealing. 
Not only does it require imposing strong assumptions on the information structures, but also it is not clear at all that markets of this complexity will be in equilibrium.
One argument is that no-regret dynamics push to the Nash Bayes set, 


Theory provides a useful guide for the structure of the mechanism and it's buidling blocks.
Further, we can 
We take a compositional approach, attempting as much as possible to reuse existing algorithms and mechanisms.



We build up to the main practical design by analysing two simplified models that illustrate the two key characteristics of our mechanism. 
First , to create the right incentives for motivating exploratory choices, we show that the rewards from the choice of action should be linked not just to the reward on the period it is taken, but to the full stream of future rewards after it.
Seccond, to aggregate signals without having the mechanism or all  experts require access to a good prior, we how offline contextual bandit algorithms and their guarantees can be used to evaluate the counterfactual value of a given experts signa.
We finish by presenting a mechanism that combines both ideas, and explore some of its limitations.

%(which can be taken as strong benchmark tha the bidding mechanisms can attempt to approximate).

% we also show that the the dynamic setting allows for the mechanism to do so without access to the prior. 

%and can use in fact incentivize the efficient aggregation of the signals form the expert population.  


% note if ou depedent on someone else info to show yours is useful, it si nto possible for strict dominant strategy for you (since when they report at random with the prior, you cant possibly do better than randomizing, no matter how many rounds we observe). 
% OTOH 


%Lykouris, http://arxiv.org/pdf/1505.00391.pdf
% The current best adaptive learning algorithm is a natural adaptation of the classical Hedge
% algorithm, AdaNormalHedge, due to Luo and Schapire (2015). With this framework in mind, we
% ask the following main question:
% How much rate of change can a system admit to sustain approximate efficiency, when its
% participants are adaptive learners?



\section{Model}


\begin{enumerate}

\item{A signal (feature) vector $\vecb{s}_{t}$ known as the
   \emph{context} or \emph{signals}.  Associated with each arm $a$ is a real-valued
   payoff $r_{t,a}\in[0,1]$ that can be related to the signals
   $\vecb{s}_{t}$ in an arbitrary way.  We denote by $S$ the set of
   contexts, and $(r_{t,1},\ldots,r_{t,K})$ the payoff vector.  
   Furthermore, assume $(\vecb{s}_t,r_{t,1},\ldots,r_{t,K})$ is drawn i.i.d. from some joint
   distribution $P$.} 
% %
 \item{Based on observed payoffs in previous trials and the current set of reportedsignals (contexts) $\vecb{s}_t$, $\Aalg$ chooses an arm $a_t\in\Aset$, and receives payoff $r_{t,a_t}$.  It is important to emphasize here that \emph{no} feedback information (namely, the payoff $r_{t,a}$) is observed for \emph{unchosen} arms $a \ne a_t$.}
\item{The algorithm then improves its arm-selection strategy with all information it observes, $(\vecb{s}_{t,a_t},a_t,r_{t,a_t})$.} \label{def:bandit-feedback}
 \end{enumerate}



% The game occurs over $T$ steps, at each step: 
% \begin{itemize}
% \item a new subject $t$ arrives and each $i$ of $K$ experts receives a signal $s_{t,i}$ for that subject.
% \item Each expert $i$ reports to the mechanism $\hat{s}_{t,i}$, and the mechanism selects a chosen action $c_t$
% \item The subject observes $c_t$, picks an action $a_t$, and receives a reward $r_t$.
% \end{itemize}

At the end of the final period the mechanism makes payments to the experts $pi_i( \cup \hat{s}_t,c_t,r_t,a_t \forall t \in T)$.


\section{A sequence of one shot mechanisms is inefficient}


Even with access to the prior by the mechanism (where the VCG styl mechanisms provide efficiency) or in situations in which the bidding mechanism is efficient in the one shot case.

Running the (single subject) efficient direct action elicitation mechanism repeatedly, results in choosing the arm with the maximum posterior expected reward at each step $t$ and using the payment rule:

\[
    \pi_i = \sum_1^T 
\begin{cases}
    \alpha (r - \expec[\hat{r}_{t,-i}]) ,& \text{if } \hat{c}_{t,-i} \neq c_t\\
    0,              & \text{otherwise}
\end{cases}
\]

With $\alpha < 1/NT$ so as to presere rational entry by the subject by limiting the payments.
The repeated single shot mechanism thus creates incentives that are equivalent to a greedy policy: it only incentivizes exploitation and not exploration.

On possible attempt to fix this would be by only revealing the outcome to the winning bidder, thus allowing them to internalize the informational advantage in future rounds payoffs. 
This  internalizies the benefits of explorations, yet it prevents the other experts from learning in those rounds when they do not win, severely limiting the situations in which the mechanism can be efficient. 

\NDP{TODO: This could use some simplification.}
\begin{eg}[3 Experts and 3 actions and 2 regimes]\label{eg:2regimes}
	Consider our model with 3 experts, 3 actions, where one of the actions $E[r|c=1]=0.51$ and the first expert recieves a signal at the start of every period with the identity of this action. For the other actions a priori $E[r|c\neq1]=0.5$ but conditional on the regime it is known that for one of them the reward are allways 0 and for the other they are always 1. The seccond and third expert on each period receive a signal that indicates if the regime has changed since the last round, but do not know the identify of the regime. Regime changes have a constant probability on any given period.
\end{eg}

Note that with access to all signals the efficient outcome involves sampling one the actions that have the $E[r|c\neq1]=0.5$ on the first round. After that point it is allways possible given the signals to identify the action with reward 1 and play it, thus in the full information aggregation case the reward is $T-1$ where T is the number of time periods. 


\begin{def}[full disclosure]
We say an optimal decision elicitation mechanism has \emph{full disclosure} if all experts recieve feedback about the value of $c$, $a$ and $r$ in every period.
\end{def}


\begin{lem}
No Exploration (in equilibrium) with full disclosure in the repeated one shot second price bidding mechanism.
\end{lem}

\begin{proof}
Example~\ref{eg:2regimes} is a witness. If we have full disclosure then if agent 1 is the highest bidder they always plays the action with expected reward $0.75$. If one of the other agents wins and chooses to explore, they gain the same information as the non-winning other agent, and in the next period both have (in expectation) the same valuation, thus the seccond price bid is the same as the first price, and the expected payments to the agent are 0, while the payment to win over the first agent must be $\alpha 0.75$ and the expected payment $\alpha 0.5033$
\end{proof}


% \begin{lem}
% No Exploration (in equilibrium) with full disclosure in the repeated one shot direct VCG mechanism.
% \end{lem}

% \begin{proof}
% Example~\ref{eg:2regimes} is a witness. If we have full disclosure then if agent 1 is the \emph{pivotal} agent (in the first period) 
% . If one of the other agents wins and chooses to explore, they gain the same information as the non-winning other agent, and in the next period both have (in expectation) the same valuation, thus the seccond price bid is 
% \end{proof}


On the other hand, if we reveal the outcome and reported signals of the round to all experts, no agent can internalize the information revelation benefits that comes from the choice of an action with a priori expected suboptimal reward.

For many problems the sequential use of mechanisms while suboptimal is not too bad inthe sense that the loss of efficincy fo the mechanism can be bounded relative to the optimum (see background chapter for a brief overvie of such results).
This is not the case for sequential optimal action elicitaiton. Repeating any efficient one shot mechanism can lead to linearly worse performance than the optimal sequential mechanism on the same problem. In Example~\ref{eg:2regimes} the regret of the  seccond price bidding mechanism is $0.25(T-1)$ since the first agent wins and selects the optimal arm given his information set.



% \begin{eg}[]
% let everyone have the same prior and signals, then upon observing the outcome, everyone has the same posterior, and thus any benefits from signals complementarity with exploration will also appear in other reports, leading the winners report not to be pivotal (change the chosen action) and thus payment 0.
% , denying the benefits form having risked a ex-ante suboptimal action to those who take it. 
% \end{eg}

% \DBA{this result (that price mechanisms can discourage exploration unless ``intellectual property'' is built in) is very nice. I imagine there's some kind of literature about this, but have no idea what formalism the IP guys would use.}
% \NDP{There is a positive externality on future time periods from exploration today, so we want to internalize those benefits for the current decision maker so as to align his incentives. It in in a line of welfare eocnomics that stretches back to 1920 formally with Pigou and the original articulation is atttributed to  Sidgwick or Marshall, but really is a bunch of 19th century economists who learned calculus more or less figured it out.

% Specifically this is a intertemporal externality which feels related to those comes form learning by doing, 
% %http://www.jstor.org/stable/2662972?seq=1#page_scan_tab_contents
% }







%\section{A Direct Mechanism which internatilizes exploration}
% TODO ? you  have to take the expected value of the future rewards with the others signals but without exploration as the baseline which to reward?



\section{A Simple Bidding Mechanism with Exploration}

To overcome this limitations of the repeated mechanism, we present a bid based mechanism that is efficient under sufficient conditions that suitably generalize those of bid based mechansims in the one shot case. 

\begin{mech}[Bidding for ownership of choice Mechanism]

An expert $i$ is the \emph{owner} at a given time period $t$ if they have won the last auction that had a winner (if no bids in a auction meet the reserve price the owner remains unchanged). 
Denote by $o_{i,t}$ a indicator variable encoding with a value of $1$ if the agent $i$ was the \emph{owner} of the choice at time $t$. 

\[
    \pi_i =  \sum_1^T
\begin{cases}
    \alpha r_t ,& \text{if } o_{i,t} = 1\\
    0,              & \text{otherwise}
\end{cases}
+
\sum_1^T
\begin{cases}
     - b_{\hat{2},t} ,& \text{if } o_{i,t} = 0 \land o_{i,t+1} = 1\\
      b_{\hat{2},t} ,& \text{if } o_{i,t}= 1 \land o_{i,t+1} = 0 \\
		0,              & \text{otherwise}
\end{cases}
\]

\end{mech}


The first part of payments sums over the rewards for all periods during which an agent owns the rights, the seccond part determines the payments, when a new agent $i$ becomes th owner, they pay out the seccond highest bid of that period, while when another agent takes over them as the owner they are payed the seccond highest bid in that period. Note that the reserve price can be encoded in the owners bid in this notation; since when it wins there is no change in owner no payments are made. 

This linking of payments address the incentive problem by internalizing the positive intertemporal information externality created by selecting actions that have not previously been selected.


Returning to Example~\ref{eg:2regimes}, this mechanism in an equilibrium has bounded regret relative to the 

\begin{lem}
No Exploration (in equilibrium) with full disclosure in the repeated one shot second price bidding mechanism.
\end{lem}

\begin{proof}
Example~\ref{eg:2regimes} is a witness. If we have full disclosure then if agent 1 is the highest bidder they always plays the action with expected reward $0.75$. If one of the other agents wins and chooses to explore, they gain the same information as the non-winning other agent, and in the next period both have (in expectation) the same valuation, thus the seccond price bid is the same as the first price, and the expected payments to the agent are 0, while the payment to win over the first agent must be $\alpha 0.75$ and the expected payment $\alpha 0.5033$
\end{proof}

%the ideal seems to be something where we just have to learn from past not aggregate. for example if all experts get the same signals and better than the subjects prior.


%prove?
%For the special case where knowledge is  fully substituble accross at least two experts informed per actions, it incentivizes the experts to carry out the optimal amount of exploration. 

%While existing results for bayesian incentive compatible exploration and its limits still apply on the subjects part if they are all rational and this is common knowledge. For medical applications this is unlikely to be the case, with a diverse set of motivations for agents making them some far from rational, and that this will be common knowledge ammong the rational agents. Somewhat paradoxically 




% \subsection{Efficiency of the mechanism}



% Bayesian/infotheoric arguments  (?)  bound the best rate at which we could learn if we had direct accss to the experts signals since the situation then reduces those signals to context and a contextual bandit lower boundon regret (TODO ref?) applies. 

% So any notion of efficiency is relative to these lower bounds. OTOH the agents priors might be much better than the mechanism and the agentsdont need to learn the mapping. 

% (i.e. given a correct prior, how fast do we identify the optimal action? how does this vary with the concentration of the prior? , the direct mechanism is equivalent to learning it, for example under Optimally Confident UCB can we give numbers? There is the 79 bounds on the minimax rate in bnadits of gittins, does that work? note dificulty of simulations, since it involves solvng for strategies that might exploit the specific algo we propose as the initiator.

%  \subsection{A Sufficient Condition for Efficiency}

%  \begin{lem}
%  One Action per Expert is not enough for incentive compatibility
%  \end{lem}

% % even when each expert observes the signal that is informative of just one action, versus when the expert observes a statistic about an action (say the mean of a given arm) which can still be informative of the mean of other arms. Note in the one shot setting this doesnt hold, and ne qctin per expert is sufficient, this is becuase the seccond price auction is effective in cuting off manipulation there, but here the effect on future beliefs and thus bids in later rounds is not cut (which in one shot it is inexistent)


%  \begin{lem}
%  Expert signals only informative about one action at one time period (and this is common knowledge) \DBA{is this a lemma or an assumption?}
%  \end{lem}


%  Note that if a signal is informative about other acions, then it allready fails at $T=1$, as we saw in the previous chapter. The basic reason for this that there is no way for the mechanism to conbine signals of different experts beyond their projection to the price space, and cmplementary signals cannot be revealed in this manner. \DBA{citation or formal explanation}

%  Notice that if the signal is informative of a single action, but that information carries through to future periods, the agent if there is some probability of not winning this period, . Thus for efficiency in thi mechanism the signal has to be local to the patient.

% Note that in the one shot setting the two notions coincide

% Both the subjects rewards are idependent conditional on the treatment, and the treatments are conditionally indepedent.
% if they are not independent one can consider a situation in which if you take my bid as truthful I can gain by lying to confuse your bid in the next period, (and i buy it in the period before or during tricking you)



\section{A Signal Revelation Mechanism Without Common Priors}

Interestingly the sequential version of the problem opens up aggregation in the repeated version of examples where it is not possible in the one shot case without in expectation paying useless experts. 
The reason why we need the prior in the one shot case to be able to evaluate the contribution the signal makes to picking an action with good rewards; since we observe a single action there is no way to estimate this without using a prior. 

We now consider a setting where the experts submit their signals, however, unlike in the .
 While we saw that there was a revelating equilibrium, the mechanism failed the property of paying for useless experts. We show that the repeated nature of the problem can be used to overcome payments to useless experts. 

We do this in two parts; by reducing the problem to a contextual bandit algorithm where the context is determined by the signals reported by all experts, and then using the randomization used by such algorithms to construct unbiased estimators of the gain in rewards brough about by a given agents signals, and making their reward proportional to this. \DBA{you need to insert a mini-survey of the contextual bandit literature. As I understand it, it's only recently (Langford's last few papers) that we've gotten to the point where contextual bandits are fully usable}

Assume that the subjects arrive IID. Use an unbiased contextual bandit algorithm evlauation strategy such as \cite{li2011unbiased}  Algorithm 2. %http://arxiv.org/pdf/1003.5956.pdf 
We then evalute algorithms that do not use a given agents signal and plug those in as estimates of 


\begin{mech}\label{mech:bandit}[Signals without Priors]


%$\hat{S}_{-i} = \bigcup \hat{s}_j  \forall j \neq i \in N $ then the expected reward given the others reports is:

After the end of the last time period, for each expert $i$, estimate the loss that would be obtained by our algorithm A without using that experts report in it's context. 


Since this is unbiased we can plug it in place of the seccon term $\expec[\sum_1^T \hat{r}_{-i,t}]$ in the payments. 

\DBA{can you say anything about controlling variance of estimates and rewards?}
\NDP{it is complicated, in theory for generic bandit algorithms no, but for specific ones yes, and this includes  epoch-greedy, UCB1, EXP3.P. (this is explained in {Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation Algorithms) I dont know about Thompson Sampling. }

Thus payment rule is announced as follows:

\[
    \pi_i =  \alpha (\sum_1^T r_t -  \expec[\sum_1^T \hat{r}_{-i,t}])
\]

Setting $\alpha<1/N$, which we can always do, bounds the total payments to the experts bellow the benefits of having the signals.

\end{mech}




Note when a agent $i$ doesnt change the exploration policy (in expectation) then , $ \expec[r_t] - \expec[\hat{r}_{-i,t}] =0  $. 

While a truthful report is not guaranteed to be strictly truthful, the bound on the bias of the plugin payent does bound how much a deviation can benefit  and this goes to zero. More practically, it appears imposible for a uninformed agent to obtain profits, and it seems very hard for a informed agent to find a strategy other than truthfulness that has higher expected reward

 \DBA{this is very loose}.


%what happened to the exploration incentives and summing over all futures? the exploraiton is taken into account by the bandit reduction, what we incentivize are reporting of signals in the form that are useful to said learner in learning the policy. 




%Note that now the winning bid agent doesnt need to have particularly good signals, but rather has to be good at aggregating the others signals; i.e. he can be the only one with access to the prior for example.
% hese two decompose nicely in the sense that the agent that is choosing, the winnr of auction, he is the one we want the logn term exploration incentives. the short term reward is internal, we want it so we know what information to stop using, but to actually make payments we can wait until the end of the mechanism.
% with "enough" time periods, we can do this

% signals reported context to a contextual bandit.
%becuase the bandit algirthm randomizing, you have a valid instrument in that randomization, i.e. they can be used to crete an unbiased (but high variance at the edges) estimator of the rewards you would have obtained with some other infromation (i.e. the paralel bandits)



\subsection{Limitations}


Note exploration implies learning how to interpret the signals, this, especially in the precence of multiple experts with conflicting priors, appears like the most practical application. I.e. not too woried about what we loose from having to randomize a little the choice, since this is needed for learing. \DBA{this is very loose. Quantify tradeoffs if possible}
This is the generalization of contextual bandits to contextual variables (signals) provided by self-interested experts who have no ingerent interest in the outcome or action, but need t be incentivated to be truthful. 
To the degree the experts know how to interpret the (full set) learning to do it is ineficient. 
How to incorporate this apears as a fruitful avenue for future research; that is how can a prior over the joint set of signals be elicited? 




% \subsection{Incentive Compatibility for Subjects}

% One natural question given the bayesian incentive compatible bandit exploration literautre, is wether these mechanisms can work when all subjects are expected utility maximizers. If the experts bring enough information to bear, the answer is yes, and it can be so without hidding past subjects outcomes. Note however,that there are intermediate situations 

%understand the bounds in http://jmlr.csail.mit.edu/proceedings/papers/v31/agrawal13a.pdf
	


%Nasty way to solve signal manipulation for future auctins This can be side steps by dividing (endogenously) the set of experts into two, and not allowing cross bidding. Open question: is there a more elegant mechanism that does this without the separation? one naural way to do the separation is to allow the experts to see the first signal, then have them self-select into the signal or aggregation pools (they go where their return is higher, we can allow them to see where others went) .


\section{A bid and signal mechanism without priors}



%The limitation of the one shot case, of having to pay for
The above signal only mechanism can be potentially ineficient when there are experts who know how to map the signals to optimal mechanisms. More broadly, experts can have extra information relative to the mechanisms that helps them aggregate the signals better but requires signals by other experts to be reported to them. 

In the previous chapter on the one shot setting, we saw that payments for  signals without a common prior that allows us to evaluate the coutnerfactual are problematic, in that we cannot reward useful experts more than useless ones. In the previous mechanism of this chapter, we saw that in the repeated setting this is not the case. We can use a unbiased estimator of what the rewards of a contextual bandit algoithm that didn't use a given agents signal could be used to fill in this counterfactual without requiring a prior.

This mechanism is the composition of the bid mechanism and signal mechanism.


\begin{mech}\label{mech:bidbandit}[]


%$\hat{S}_{-i} = \bigcup \hat{s}_j  \forall j \neq i \in N $ then the expected reward given the others reports is:

The payment rule is announced. In each period: all agents report signals and bids to the mechanism, the mechanism displays the other experts signals to the winner of the bidding, the winner selects the chosen action $c_t$, this is displayed to the subject, who takes action $a_t$ and receives reward $r_t$.

After the end of the last time period, for each expert $i$, estimate the loss that would be obtained by a no regret algorithm without using that experts report in it's context, and use this as $\expec[\sum_1^T \hat{r}_{-i,t}]$ in the payments. The choice part of the payments is identical to the bidding.


Thus payment rule thus is:


\[
    \pi_i =  \sum_1^T
\begin{cases}
    \alpha r_t ,& \text{if } o_{i,t} = 1\\
     0,              & \text{otherwise}
\end{cases}
+
\sum_1^T
\begin{cases}
     - b_{\hat{2},t} ,& \text{if } o_{i,t} = 0 \land o_{i,t+1} = 1\\
       b_{\hat{2},t} ,& \text{if } o_{i,t} = 1 \land o_{i,t+1} = 0 \\
	   0,              & \text{otherwise}
\end{cases}
  +  \beta \sum_1^T r_t -  \expec[\sum_1^T \hat{r}_{-i,t}]
\]

 

Where $\alpha$ and $\beta$ are again set a priori. Given we are attempting not to use a prior distribution, using  constants such that  $\alpha + \beta < 1/2NT$ .

\end{mech}

 A major concern is that a expert would not reveal their signal turthfully and loose out on that part of the reward, if they can benefit more from being the \emph{owner} and by widtholding their signal they can supress bids of othe experts, this is a particular conceern since the other experts may be able to achieve higher rewards.

 concerns: the experts knwo the estimator you are using, can out bid you and then make the chocies so you realized bias is high. One way to try to avoid this would be to take out the rounds where the expert $i$ won the auction when calculating the signal value of $i$, note this is biased (it could be that those are when he brings a lot of value to both the signal and the aggregation of signals,a dn this is quite natural)






